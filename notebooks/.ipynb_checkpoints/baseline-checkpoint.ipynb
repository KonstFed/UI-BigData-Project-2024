{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd7560a-e38d-4af3-bf1d-bb6c7644958d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ecb87b-aab7-40de-bfd5-eb335e9e8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse = \"/user/team20/project/hive/warehouse\"\n",
    "team = \"team20\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"{} - spark ML\".format(team))\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea10a058-e357-4223-8bb0-dd6456dac44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|             default|\n",
      "|             root_db|\n",
      "|     team0_projectdb|\n",
      "|team12_hive_proje...|\n",
      "|    team13_projectdb|\n",
      "|    team14_projectdb|\n",
      "|    team15_projectdb|\n",
      "|    team16_projectdb|\n",
      "|    team18_projectdb|\n",
      "|    team19_projectdb|\n",
      "|     team1_projectdb|\n",
      "|    team20_projectdb|\n",
      "|    team21_projectdb|\n",
      "|    team22_projectdb|\n",
      "|    team23_projectdb|\n",
      "|    team25_projectdb|\n",
      "|    team26_projectdb|\n",
      "|    team27_projectdb|\n",
      "|    team28_projectdb|\n",
      "|    team29_projectdb|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+----------------+---------------+-----------+\n",
      "|       namespace|      tableName|isTemporary|\n",
      "+----------------+---------------+-----------+\n",
      "|team20_projectdb|anime_part_buck|      false|\n",
      "|team20_projectdb|     q1_results|      false|\n",
      "|team20_projectdb|     q2_results|      false|\n",
      "|team20_projectdb|     q3_results|      false|\n",
      "|team20_projectdb|     q4_results|      false|\n",
      "|team20_projectdb|     q5_results|      false|\n",
      "|team20_projectdb|  users_details|      false|\n",
      "|team20_projectdb|   users_scores|      false|\n",
      "+----------------+---------------+-----------+\n",
      "\n",
      "+-----+--------------------+-----+--------------------+--------------------+-------+--------+---------+--------------------+-----------+--------------+---------+-----------------+-----+----------+---------+---------+-------+--------------------+\n",
      "|   id|               title|score|              genres|            synopsis|   type|episodes|is_airing|           producers|  licensors|       studios|   source|         duration| rank|popularity|favorites|scored_by|members|              rating|\n",
      "+-----+--------------------+-----+--------------------+--------------------+-------+--------+---------+--------------------+-----------+--------------+---------+-----------------+-----+----------+---------+---------+-------+--------------------+\n",
      "|55646|         castlevania| -1.0|['drama', 'horror...|a vampire slayer ...|     tv|      32|    false|                   -|          -|             -|    manga|    20 min per ep|    0|         0|        0|       -1|      0|R - 17+ (violence...|\n",
      "|55266|    wang shen zhi ye| -1.0|['horror', 'super...|on a full moon ni...|  movie|       1|    false|                   -|          -|             -| original|           29 min|13312|     24659|        0|       -1|     27|R - 17+ (violence...|\n",
      "|54287|kun bing zhi long pv| -1.0|['suspense', 'dra...|promotional video...|    ona|       1|    false|                   -|          -|             -|web manga|            1 min|17550|     22318|        0|       -1|     58|R - 17+ (violence...|\n",
      "|53155|dokuhaku suru uni...| -1.0|['suspense', 'mys...|a psychopathic ki...|    ova|       1|    false|                   -|          -|             -|    novel|           42 min|15616|     20881|        0|       -1|     85|R - 17+ (violence...|\n",
      "|52747|psycho-pass movie...| 7.01|['mystery', 'susp...|                   -|  movie|       1|    false|sony pictures ent...|crunchyroll|production i.g| original|             2 hr| 3958|      3590|      110|      142|  33428|R - 17+ (violence...|\n",
      "|52494|              bao mu| -1.0|        ['suspense']|a female killer, ...|  movie|       1|    false|                   -|          -|             -| original|            9 min|14809|     18845|        0|       -1|    200|R - 17+ (violence...|\n",
      "|52032|            gaksital| -1.0| ['sports', 'drama']|                   -|  movie|       1|    false|        daewon media|          -|             -|    manga|       1 hr 6 min|15996|     20539|        0|       -1|    102|R - 17+ (violence...|\n",
      "|51693|kaminaki sekai no...| 6.79|['comedy', 'fanta...|under the belief ...|     tv|      12|     true|lantis, dax produ...|          -|studio palette|    manga|    23 min per ep| 4914|      1818|      413|    21318| 110162|R - 17+ (violence...|\n",
      "|51640|zui ke ai de ren ...| -1.0|               ['-']|                   -|  movie|       1|    false|huaxia film distr...|          -|             -|  unknown|      1 hr 24 min|14390|     20684|        0|       -1|     94|R - 17+ (violence...|\n",
      "|51535|shingeki no kyoji...| 9.05|['suspense', 'act...|in the wake of er...|special|       2|     true|pony canyon, koda...|          -|         mappa|    manga|1 hr 1 min per ep|    7|       479|     9078|   155773| 435672|R - 17+ (violence...|\n",
      "+-----+--------------------+-----+--------------------+--------------------+-------+--------+---------+--------------------+-----------+--------------+---------+-----------------+-----+----------+---------+---------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "spark.sql(\"USE team20_projectdb\").show()\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "spark.sql(\"SELECT * FROM anime_part_buck LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77a57a2a-2ec7-4e7e-9a71-15752ed3b2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='anime_part_buck', database='team20_projectdb', description=None, tableType='EXTERNAL', isTemporary=False), Table(name='q1_results', database='team20_projectdb', description=None, tableType='EXTERNAL', isTemporary=False), Table(name='q2_results', database='team20_projectdb', description=None, tableType='EXTERNAL', isTemporary=False), Table(name='q3_results', database='team20_projectdb', description=None, tableType='EXTERNAL', isTemporary=False), Table(name='q4_results', database='team20_projectdb', description=None, tableType='EXTERNAL', isTemporary=False), Table(name='q5_results', database='team20_projectdb', description=None, tableType='EXTERNAL', isTemporary=False), Table(name='users_details', database='team20_projectdb', description=None, tableType='EXTERNAL', isTemporary=False), Table(name='users_scores', database='team20_projectdb', description=None, tableType='EXTERNAL', isTemporary=False)]\n"
     ]
    }
   ],
   "source": [
    "print(spark.catalog.listTables(\"team20_projectdb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "156b70bd-8d49-44b3-b9c4-72b78a10e195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|23796586|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT count(*) FROM users_scores\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b14fae40-9bdb-478f-af9b-e5e0b7c5f011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+\n",
      "|user_id|anime_id|rating|\n",
      "+-------+--------+------+\n",
      "|      1|      21|     9|\n",
      "|      1|      48|     7|\n",
      "|      1|     320|     5|\n",
      "|      1|      49|     8|\n",
      "|      1|     304|     8|\n",
      "|      1|     306|     8|\n",
      "|      1|      53|     7|\n",
      "|      1|      47|     5|\n",
      "|      1|     591|     6|\n",
      "|      1|      54|     7|\n",
      "|      1|      55|     5|\n",
      "|      1|      56|     6|\n",
      "|      1|      57|     9|\n",
      "|      1|     368|     5|\n",
      "|      1|      68|     7|\n",
      "|      1|     889|     9|\n",
      "|      1|    1519|     7|\n",
      "|      1|      58|     8|\n",
      "|      1|    1222|     7|\n",
      "|      1|     458|     4|\n",
      "+-------+--------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data = spark.sql(\"SELECT user_id, anime_id, rating FROM users_scores\")\n",
    "data = spark.sql(\"SELECT user_id, anime_id, rating FROM users_scores ORDER BY user_id LIMIT 5000\")\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07371f27-5673-4d27-b89d-9e6f0ed6e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# inspired by https://medium.com/@brunoborges_38708/recommender-system-using-als-in-pyspark-10329e1d1ee1\n",
    "\n",
    "user_window = Window.partitionBy(\"user_id\").orderBy(F.col(\"anime_id\").desc())\n",
    "\n",
    "df_rec_filtered = data.withColumn(\"num_items\", F.expr(\"count(*) over (partition by user_id)\"))\n",
    "df_rec_filtered = df_rec_filtered.filter(F.col(\"num_items\")>=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8c3e377-a7b4-4b09-8e52-96da60e9a881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514 1483\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "\n",
    "\n",
    "# For example, 30% of items will be masked\n",
    "percent_items_to_mask = 0.3\n",
    "\n",
    "# Determine the number of items to mask for each user\n",
    "df_rec_final = df_rec_filtered.withColumn(\"num_items_to_mask\", (F.col(\"num_items\") * percent_items_to_mask).cast(\"int\"))\n",
    "\n",
    "# Masks items for each user\n",
    "# _tmp_window = Window.partitionBy(\"user_id\").orderBy(F.rand(seed))\n",
    "df_rec_final = df_rec_final.withColumn(\"item_rank\", F.rank().over(user_window))\n",
    "\n",
    "# Create a StringIndexer model to index the user ID column\n",
    "# indexer_user = StringIndexer(inputCol='user_id', outputCol='userIndex').setHandleInvalid(\"keep\")\n",
    "# indexer_item = StringIndexer(inputCol='anime_id', outputCol='itemIndex').setHandleInvalid(\"keep\")\n",
    "\n",
    "# # Fit the indexer model to the data and transform the DataFrame\n",
    "# df_rec_final = indexer_user.fit(df_rec_final).transform(df_rec_final)\n",
    "# df_rec_final = indexer_item.fit(df_rec_final).transform(df_rec_final)\n",
    "\n",
    "# Convert the userIndex column to integer type\n",
    "# df_rec_final = df_rec_final.withColumn('userIndex', df_rec_final['userIndex'].cast('integer')) \\\n",
    "#     .withColumn('itemIndex', df_rec_final['itemIndex'].cast('integer'))\n",
    "\n",
    "# Filter train and test DataFrames\n",
    "train_df_rec = df_rec_final.filter(F.col(\"item_rank\") > F.col(\"num_items_to_mask\"))\n",
    "test_df_rec = df_rec_final.filter(F.col(\"item_rank\") <= F.col(\"num_items_to_mask\"))\n",
    "print(train_df_rec.count(), test_df_rec.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc276064-b09f-440f-9c94-8ebcdb1770e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_rec.write.json(\"/user/team20/project/data/train.json\")\n",
    "test_df_rec.write.json(\"/user/team20/project/data/test.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8e264da-313c-4971-a2f1-16f28bc00abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3514 1483\n"
     ]
    }
   ],
   "source": [
    "top_n_user = train_df_rec.groupBy(\"user_id\").agg(F.count(\"rating\").alias(\"n_watched\")).orderBy(F.desc(\"n_watched\")).limit(1000)\n",
    "train_df_rec = train_df_rec.join(top_n_user, on=\"user_id\", how=\"inner\")\n",
    "test_df_rec = test_df_rec.join(top_n_user, on=\"user_id\", how=\"inner\")\n",
    "\n",
    "print(train_df_rec.count(), test_df_rec.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aacb45b7-ca2d-44ba-9304-1c5f7126fe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|user_id|n_watched|\n",
      "+-------+---------+\n",
      "|    119|      285|\n",
      "|    162|      208|\n",
      "|      4|      199|\n",
      "|     23|      198|\n",
      "|      1|      184|\n",
      "|    185|      176|\n",
      "|    138|      159|\n",
      "|    208|      159|\n",
      "|    133|      147|\n",
      "|    157|      145|\n",
      "|     66|      139|\n",
      "|     70|      126|\n",
      "|     82|      116|\n",
      "|    222|      112|\n",
      "|    212|      108|\n",
      "|     47|      101|\n",
      "|     95|       89|\n",
      "|    112|       89|\n",
      "|    120|       88|\n",
      "|    143|       82|\n",
      "+-------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_n_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffcdda58-9d8c-456c-af40-c797f13f21fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "# def does_contain_same(train: pyspark.sql.dataframe.DataFrame, test: pyspark.sql.dataframe.DataFrame, column: str) -> bool:\n",
    "#     tr_dist = train.select(column).distinct().orderBy(F.col(column).asc())\n",
    "#     test_dist = test.select(column).distinct().orderBy(F.col(column).asc())\n",
    "#     return tr_dist.collect() == test_dist.collect()\n",
    "\n",
    "# does_contain_same(train_df_rec, test_df_rec, \"user_id\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cb1f7f9-6c49-4da7-8bc9-1fb9dfd0adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rank:  30\n",
      "MaxIter:  20\n",
      "RegParam:  0.15\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "\n",
    "als = ALS(userCol='user_id', itemCol='anime_id', ratingCol='rating',\n",
    "          coldStartStrategy='drop')\n",
    "\n",
    "# param_grid = ParamGridBuilder()\\\n",
    "#              .addGrid(als.rank, [1, 20, 30])\\\n",
    "#              .addGrid(als.maxIter, [10, 20])\\\n",
    "#              .addGrid(als.regParam, [.05, .15])\\\n",
    "#              .build()\n",
    "\n",
    "param_grid = ParamGridBuilder()\\\n",
    "             .addGrid(als.rank, [5, 20])\\\n",
    "             .addGrid(als.maxIter, [10, 20])\\\n",
    "             .addGrid(als.regParam, [.05, .15])\\\n",
    "             .build()\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='rating', predictionCol='prediction')\n",
    "\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "model = cv.fit(train_df_rec)\n",
    "best_model = model.bestModel\n",
    "\n",
    "print('rank: ', best_model.rank)\n",
    "print('MaxIter: ', best_model._java_obj.parent().getMaxIter())\n",
    "print('RegParam: ', best_model._java_obj.parent().getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68357d05-481b-48be-b151-febc8ac2344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "als = ALS(rank=best_model.rank, maxIter=best_model._java_obj.parent().getMaxIter(), regParam=best_model._java_obj.parent().getRegParam(), userCol=\"user_id\", itemCol=\"anime_id\")\n",
    "model = als.fit(train_df_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e957f364-bd56-4ebf-abd4-acbc1d221fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     recommendations|\n",
      "+--------------------+\n",
      "|[{44, 9.748807}, ...|\n",
      "|[{114, 9.630049},...|\n",
      "|[{21, 10.22297}, ...|\n",
      "|[{1142, 9.574842}...|\n",
      "|[{2593, 9.854924}...|\n",
      "|[{122, 9.226507},...|\n",
      "|[{75, 9.916822}, ...|\n",
      "|[{44, 9.578612}, ...|\n",
      "|[{1889, 9.586806}...|\n",
      "|[{777, 9.97529}, ...|\n",
      "|[{164, 8.844166},...|\n",
      "|[{121, 10.919873}...|\n",
      "|[{92, 9.866216}, ...|\n",
      "|[{1142, 9.668254}...|\n",
      "|[{121, 11.036204}...|\n",
      "|[{1689, 9.72442},...|\n",
      "|[{467, 9.658599},...|\n",
      "|[{21, 9.651628}, ...|\n",
      "|[{801, 9.856971},...|\n",
      "|[{302, 9.871355},...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.recommendForAllUsers(10)\n",
    "predictions.select(\"recommendations\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71cb34f1-9774-402b-b23f-5d58d125d541",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER2RECOMMEND = best_model.rank\n",
    "\n",
    "most_rated_anime = df_rec_filtered.groupBy(\"anime_id\")\\\n",
    "                    .agg(F.mean(\"rating\").alias(\"avg_rating\"), F.count(\"rating\").alias(\"count\"))\\\n",
    "                    .where(\"avg_rating > 8\")\\\n",
    "                    .orderBy(F.desc(\"count\")).limit(NUMBER2RECOMMEND)\n",
    "most_rated_anime = most_rated_anime.select(\"anime_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5541ed01-060d-4bb3-a017-0c807b636cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|     recommendations|\n",
      "+-------+--------------------+\n",
      "|     20|[44, 1827, 121, 1...|\n",
      "|    120|[114, 139, 138, 1...|\n",
      "|    130|[21, 136, 44, 121...|\n",
      "|     80|[1142, 16, 121, 1...|\n",
      "|     70|[2593, 1072, 67, ...|\n",
      "|     91|[122, 121, 19, 21...|\n",
      "|    111|[75, 121, 30, 245...|\n",
      "|      1|[44, 199, 136, 26...|\n",
      "|     71|[1889, 431, 160, ...|\n",
      "|    222|[777, 282, 264, 3...|\n",
      "|    212|[164, 593, 75, 44...|\n",
      "|    112|[121, 1689, 1889,...|\n",
      "|    162|[92, 2457, 1142, ...|\n",
      "|     82|[1142, 16, 489, 2...|\n",
      "|     53|[121, 16, 1889, 1...|\n",
      "|    133|[1689, 953, 121, ...|\n",
      "|    223|[467, 164, 199, 4...|\n",
      "|    163|[21, 122, 269, 26...|\n",
      "|     23|[801, 467, 777, 2...|\n",
      "|    143|[302, 283, 121, 7...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = predictions.withColumn(\"recommendations\", F.transform(F.col('recommendations'), lambda x: x.anime_id))\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "424526ba-28e4-4a53-82cd-22ff63f02abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|user_id|                  gt|\n",
      "+-------+--------------------+\n",
      "|      1|[1698, 849, 32281...|\n",
      "|      4|[31043, 38003, 33...|\n",
      "|      9|[317, 304, 392, 3...|\n",
      "|     20|[11061, 9253, 364...|\n",
      "|     23|[22535, 22199, 18...|\n",
      "|     47|[6351, 4059, 2963...|\n",
      "|     53|[48411, 42847, 41...|\n",
      "|     66|[5680, 4224, 3958...|\n",
      "|     70|[8074, 5258, 5114...|\n",
      "|     71|[5258, 2890, 1535...|\n",
      "|     80|[4224, 4483, 4477...|\n",
      "|     82|[36296, 35120, 37...|\n",
      "|     83|[16774, 30240, 20...|\n",
      "|     91|[995, 4053, 2752,...|\n",
      "|     95|[28771, 22507, 22...|\n",
      "|    108|[4177, 3572, 3588...|\n",
      "|    111|[529, 433, 390, 2...|\n",
      "|    112|[38524, 38000, 35...|\n",
      "|    116|[323, 345, 269, 2...|\n",
      "|    119|[20785, 19815, 19...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_rec_list = test_df_rec.select(\"user_id\", \"anime_id\", \"rating\").orderBy(\"user_id\", F.desc(\"rating\")).groupBy(\"user_id\").agg(F.collect_list(\"anime_id\").alias(\"gt\"))\n",
    "test_rec_list.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "422108c2-055b-48d5-b69b-ef91a2645037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 36)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.count(), test_rec_list.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bc6d482-97d6-4d5c-a064-f35a5ba52955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|user_id|                  gt|     recommendations|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|[1698, 849, 32281...|[44, 199, 136, 26...|\n",
      "|      4|[31043, 38003, 33...|[3901, 3784, 4725...|\n",
      "|      9|[317, 304, 392, 3...|[44, 226, 72, 139...|\n",
      "|     20|[11061, 9253, 364...|[44, 1827, 121, 1...|\n",
      "|     23|[22535, 22199, 18...|[801, 467, 777, 2...|\n",
      "|     47|[6351, 4059, 2963...|[1689, 1530, 256,...|\n",
      "|     53|[48411, 42847, 41...|[121, 16, 1889, 1...|\n",
      "|     66|[5680, 4224, 3958...|[75, 106, 98, 232...|\n",
      "|     70|[8074, 5258, 5114...|[2593, 1072, 67, ...|\n",
      "|     71|[5258, 2890, 1535...|[1889, 431, 160, ...|\n",
      "|     80|[4224, 4483, 4477...|[1142, 16, 121, 1...|\n",
      "|     82|[36296, 35120, 37...|[1142, 16, 489, 2...|\n",
      "|     83|[16774, 30240, 20...|[121, 1142, 3784,...|\n",
      "|     91|[995, 4053, 2752,...|[122, 121, 19, 21...|\n",
      "|     95|[28771, 22507, 22...|[1894, 1889, 413,...|\n",
      "|    108|[4177, 3572, 3588...|[306, 3358, 656, ...|\n",
      "|    111|[529, 433, 390, 2...|[75, 121, 30, 245...|\n",
      "|    112|[38524, 38000, 35...|[121, 1689, 1889,...|\n",
      "|    116|[323, 345, 269, 2...|[44, 5, 19, 1142,...|\n",
      "|    119|[20785, 19815, 19...|[12531, 5040, 611...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recomendations = test_rec_list.join(predictions, on=\"user_id\", how=\"inner\")\n",
    "recomendations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e448ecf0-8da7-44b1-b61e-b92c7e171ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "recomendations.write.parquet(\"/user/team20/project/output/model1_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aebe87dc-cd78-4332-9cb4-3e3dfad7c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"/user/team20/project/models/model1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e63b7d4-38c4-4cce-a435-0151bd263ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "\n",
    "metrics = RankingMetrics(recomendations.select(\"gt\", \"recommendations\").rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a43c295d-563c-405f-904e-49b9e40499f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, FloatType\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"model\", StringType(), True),\n",
    "    StructField(\"precision@10\", FloatType(), True),\n",
    "    StructField(\"recall@10\", FloatType(), True),\n",
    "    StructField(\"ndcg@10\", FloatType(), True),\n",
    "    StructField(\"precision@5\", FloatType(), True),\n",
    "    StructField(\"recall@5\", FloatType(), True),\n",
    "    StructField(\"ndcg@5\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Create a list of tuples with sample data\n",
    "data = [(\n",
    "    \"avg_synopsis_emb\",\n",
    "    metrics.precisionAt(10),\n",
    "    metrics.recallAt(10),\n",
    "    metrics.ndcgAt(10),\n",
    "    metrics.precisionAt(5),\n",
    "    metrics.recallAt(5),\n",
    "    metrics.ndcgAt(5)\n",
    ")]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.write.parquet(\"/user/team20/project/output/evaluation\", mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "448baf8d-401d-47fc-a474-2cdc56f826f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 55272)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib64/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.6/site-packages/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/local/lib/python3.6/site-packages/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.6/site-packages/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with open(\"/home/team20/team20/bigdata-final-project-iu-2024.git/output/evaluation.csv\", \"w\") as f:\n",
    "    f.write(\"model,precision@10,recall@10,ndcg@10,precision@5,recall@5,ndcg@5\\n\")\n",
    "    f.write(\"ALS,\")\n",
    "    for v in [10, 5]:\n",
    "        f.write(\",\".join(map(str, [metrics.precisionAt(v), metrics.recallAt(v), metrics.ndcgAt(v)])))\n",
    "        if v == 10:\n",
    "            f.write(\",\")\n",
    "    f.write(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a35b5789-ce2e-42a7-a6d7-979902797663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_rated_anime_list = most_rated_anime.rdd.map(lambda x: x.anime_id).collect()\n",
    "\n",
    "# most_rated_anime_df = test_rec_list.select(\"user_id\", \"gt\")\n",
    "# most_rated_anime_df = most_rated_anime_df.withColumn(\"recommendations\", F.array([F.lit(x) for x in most_rated_anime_list]))\n",
    "# most_rated_anime_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70585366-0d0c-4086-b51d-5c9bd5f08f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_popular_metrics = RankingMetrics(most_rated_anime_df.select(\"gt\", \"recommendations\").rdd)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86f02d4e-8592-41ed-93d8-e9dd01d1f158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_popular_metrics.ndcgAt(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d10fe946-622a-403e-a6c1-12d32a36738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_popular_metrics.precisionAt(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
